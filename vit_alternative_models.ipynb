{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vit-alternative-models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyORvi2Cr/DKQXG5xsWEN3V2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WilliamAshbee/3d-synth-data/blob/main/vit_alternative_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIVPnF4SkSzf"
      },
      "source": [
        "#https://github.com/lucidrains/vit-pytorch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-c3aAUOfow0",
        "outputId": "ea29c1fb-616f-4f9d-e7fc-dc63b75823d5"
      },
      "source": [
        "!pip install vit_pytorch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vit_pytorch in /usr/local/lib/python3.7/dist-packages (0.21.1)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from vit_pytorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from vit_pytorch) (0.11.1+cu111)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.7/dist-packages (from vit_pytorch) (0.3.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->vit_pytorch) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->vit_pytorch) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->vit_pytorch) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT16z-A9fWxY",
        "outputId": "fad9ec09-f65a-46c7-833c-72959c1b7941"
      },
      "source": [
        "import torch\n",
        "from vit_pytorch import ViT\n",
        "\n",
        "v = ViT(\n",
        "    image_size = 32,\n",
        "    patch_size = 8,\n",
        "    num_classes = 2000,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 32, 32)\n",
        "\n",
        "preds = v(img) # (1, 1000)\n",
        "print('preds',preds.shape)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preds torch.Size([1, 2000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4YF6OU_fiLz"
      },
      "source": [
        "# import torch\n",
        "# from torchvision.models import resnet50\n",
        "\n",
        "# from vit_pytorch.distill import DistillableViT, DistillWrapper\n",
        "\n",
        "# teacher = resnet50(pretrained = True)\n",
        "\n",
        "# v = DistillableViT(\n",
        "#     image_size = 32,\n",
        "#     patch_size = 8,\n",
        "#     num_classes = 2000,\n",
        "#     dim = 1024,\n",
        "#     depth = 8,\n",
        "#     heads = 16,\n",
        "#     mlp_dim = 2048,\n",
        "#     dropout = 0.1,\n",
        "#     emb_dropout = 0.1\n",
        "# )\n",
        "\n",
        "# distiller = DistillWrapper(\n",
        "#     student = v,\n",
        "#     teacher = teacher,\n",
        "#     temperature = 3,           # temperature of distillation\n",
        "#     alpha = 0.5,               # trade between main loss and distillation loss\n",
        "#     hard = False               # whether to use soft or hard distillation\n",
        "# )\n",
        "\n",
        "# img = torch.randn(2, 3, 32, 32)\n",
        "# labels = torch.randint(0, 2000, (2,))\n",
        "\n",
        "# loss = distiller(img, labels)\n",
        "# loss.backward()\n",
        "\n",
        "# # after lots of training above ...\n",
        "\n",
        "# pred = v(img) # (2, 1000)\n",
        "# print('preds',preds.shape)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFyBCFrqf7_H",
        "outputId": "3a721e76-4983-404e-ec18-06134458e032"
      },
      "source": [
        "import torch\n",
        "from vit_pytorch.deepvit import DeepViT\n",
        "\n",
        "v = DeepViT(\n",
        "    image_size = 32,\n",
        "    patch_size = 8,\n",
        "    num_classes = 1000,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 32, 32)\n",
        "\n",
        "preds = v(img) # (1, 1000)\n",
        "print('preds',preds.shape)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preds torch.Size([1, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KvtstXTgSXo",
        "outputId": "629629bb-c10d-408b-a768-0a3335218b99"
      },
      "source": [
        "import torch\n",
        "from vit_pytorch.cait import CaiT\n",
        "\n",
        "v = CaiT(\n",
        "    image_size = 32,\n",
        "    patch_size = 8,\n",
        "    num_classes = 2000,\n",
        "    dim = 1024,\n",
        "    depth = 12,             # depth of transformer for patch to patch attention only\n",
        "    cls_depth = 2,          # depth of cross attention of CLS tokens to patch\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1,\n",
        "    layer_dropout = 0.05    # randomly dropout 5% of the layers\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 32, 32)\n",
        "\n",
        "preds = v(img) # (1, 1000)\n",
        "print('preds',preds.shape)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preds torch.Size([1, 2000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iI9iTY9-geyJ",
        "outputId": "3542f82f-0cc8-43af-f2b2-162165d12a89"
      },
      "source": [
        "import torch\n",
        "from vit_pytorch.t2t import T2TViT\n",
        "\n",
        "v = T2TViT(\n",
        "    dim = 512,\n",
        "    image_size = 32,\n",
        "    depth = 8,\n",
        "    heads = 16,\n",
        "    mlp_dim = 512,\n",
        "    num_classes = 2000,\n",
        "    t2t_layers = ((7, 4), (3, 2), (3, 2)) # tuples of the kernel size and stride of each consecutive layers of the initial token to token module\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 32, 32)\n",
        "\n",
        "preds = v(img) # (1, 1000)\n",
        "print('preds',preds.shape)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preds torch.Size([1, 2000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA34tFp8gmLZ"
      },
      "source": [
        "# import torch\n",
        "# from vit_pytorch.cct import CCT\n",
        "\n",
        "# model = CCT(\n",
        "#         img_size=32,\n",
        "#         embedding_dim=384,\n",
        "#         n_conv_layers=2,\n",
        "#         kernel_size=7,\n",
        "#         stride=2,\n",
        "#         padding=3,\n",
        "#         pooling_kernel_size=3,\n",
        "#         pooling_stride=2,\n",
        "#         pooling_padding=1,\n",
        "#         num_layers=14,\n",
        "#         num_heads=6,\n",
        "#         mlp_radio=3.,\n",
        "#         num_classes=2000,\n",
        "#         positional_embedding='learnable', # ['sine', 'learnable', 'none']\n",
        "#         )\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM6LzWzcgyJi"
      },
      "source": [
        "# import torch\n",
        "# from vit_pytorch.cct import cct_14\n",
        "\n",
        "# model = cct_14(\n",
        "#         img_size=32,\n",
        "#         n_conv_layers=1,\n",
        "#         kernel_size=7,\n",
        "#         stride=2,\n",
        "#         padding=3,\n",
        "#         pooling_kernel_size=3,\n",
        "#         pooling_stride=2,\n",
        "#         pooling_padding=1,\n",
        "#         num_classes=2000,\n",
        "#         positional_embedding='learnable', # ['sine', 'learnable', 'none']  \n",
        "#         )"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVT8MKvSg21b",
        "outputId": "0cd1a69d-0f21-4e3a-fcab-71a8dc68ef1c"
      },
      "source": [
        "import torch\n",
        "from vit_pytorch.cross_vit import CrossViT\n",
        "\n",
        "v = CrossViT(\n",
        "    image_size = 32,\n",
        "    num_classes = 2000,\n",
        "    depth = 4,               # number of multi-scale encoding blocks\n",
        "    sm_dim = 192,            # high res dimension\n",
        "    sm_patch_size = 8,      # high res patch size (should be smaller than lg_patch_size)\n",
        "    sm_enc_depth = 2,        # high res depth\n",
        "    sm_enc_heads = 8,        # high res heads\n",
        "    sm_enc_mlp_dim = 2048,   # high res feedforward dimension\n",
        "    lg_dim = 384,            # low res dimension\n",
        "    lg_patch_size = 8,      # low res patch size\n",
        "    lg_enc_depth = 3,        # low res depth\n",
        "    lg_enc_heads = 8,        # low res heads\n",
        "    lg_enc_mlp_dim = 2048,   # low res feedforward dimensions\n",
        "    cross_attn_depth = 2,    # cross attention rounds\n",
        "    cross_attn_heads = 8,    # cross attention heads\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 32, 32)\n",
        "\n",
        "pred = v(img) # (1, 1000)\n",
        "print('preds',preds.shape)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preds torch.Size([1, 2000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VvGWLRHg9k5",
        "outputId": "5c4a953a-3dc7-4035-bf14-f638d6ecb84f"
      },
      "source": [
        "import torch\n",
        "from vit_pytorch.pit import PiT\n",
        "\n",
        "v = PiT(\n",
        "    image_size = 32,\n",
        "    patch_size = 8,\n",
        "    dim = 256,\n",
        "    num_classes = 2000,\n",
        "    depth = (3, 3, 3),     # list of depths, indicating the number of rounds of each stage before a downsample\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")\n",
        "\n",
        "# forward pass now returns predictions and the attention maps\n",
        "\n",
        "img = torch.randn(1, 3, 32, 32)\n",
        "\n",
        "preds = v(img) # (1, 1000)\n",
        "print('preds',preds.shape)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preds torch.Size([1, 2000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8pGV3BuhHDF"
      },
      "source": [
        "# import torch\n",
        "# from vit_pytorch.levit import LeViT\n",
        "\n",
        "# levit = LeViT(\n",
        "#     image_size = 32,\n",
        "#     num_classes = 2000,\n",
        "#     stages = 3,             # number of stages\n",
        "#     dim = (32, 64, 128),  # dimensions at each stage\n",
        "#     depth = 4,              # transformer of depth 4 at each stage\n",
        "#     heads = (4, 6, 8),      # heads at each stage\n",
        "#     mlp_mult = 2,\n",
        "#     dropout = 0.1\n",
        "# )\n",
        "\n",
        "# img = torch.randn(1, 3, 32, 32)\n",
        "\n",
        "# preds = levit(img) # (1, 1000)\n",
        "# print('preds',preds.shape)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J0rE8PRhOL1"
      },
      "source": [
        "# import torch\n",
        "# from vit_pytorch.cvt import CvT\n",
        "\n",
        "# v = CvT(\n",
        "#     num_classes = 2000,\n",
        "#     s1_emb_dim = 64,        # stage 1 - dimension\n",
        "#     s1_emb_kernel = 7,      # stage 1 - conv kernel\n",
        "#     s1_emb_stride = 4,      # stage 1 - conv stride\n",
        "#     s1_proj_kernel = 3,     # stage 1 - attention ds-conv kernel size\n",
        "#     s1_kv_proj_stride = 2,  # stage 1 - attention key / value projection stride\n",
        "#     s1_heads = 1,           # stage 1 - heads\n",
        "#     s1_depth = 1,           # stage 1 - depth\n",
        "#     s1_mlp_mult = 4,        # stage 1 - feedforward expansion factor\n",
        "#     s2_emb_dim = 192,       # stage 2 - (same as above)\n",
        "#     s2_emb_kernel = 3,\n",
        "#     s2_emb_stride = 2,\n",
        "#     s2_proj_kernel = 3,\n",
        "#     s2_kv_proj_stride = 2,\n",
        "#     s2_heads = 3,\n",
        "#     s2_depth = 2,\n",
        "#     s2_mlp_mult = 4,\n",
        "#     s3_emb_dim = 384,       # stage 3 - (same as above)\n",
        "#     s3_emb_kernel = 3,\n",
        "#     s3_emb_stride = 2,\n",
        "#     s3_proj_kernel = 3,\n",
        "#     s3_kv_proj_stride = 2,\n",
        "#     s3_heads = 4,\n",
        "#     s3_depth = 10,\n",
        "#     s3_mlp_mult = 4,\n",
        "#     dropout = 0\n",
        "# )\n",
        "\n",
        "# img = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# pred = v(img) # (1, 1000)\n",
        "# print('preds',preds.shape)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwxwnk3Yha0x"
      },
      "source": [
        "# import torch\n",
        "# from vit_pytorch.twins_svt import TwinsSVT\n",
        "\n",
        "# model = TwinsSVT(\n",
        "#     num_classes = 2000,       # number of output classes\n",
        "#     s1_emb_dim = 64,          # stage 1 - patch embedding projected dimension\n",
        "#     s1_patch_size = 4,        # stage 1 - patch size for patch embedding\n",
        "#     s1_local_patch_size = 7,  # stage 1 - patch size for local attention\n",
        "#     s1_global_k = 7,          # stage 1 - global attention key / value reduction factor, defaults to 7 as specified in paper\n",
        "#     s1_depth = 1,             # stage 1 - number of transformer blocks (local attn -> ff -> global attn -> ff)\n",
        "#     s2_emb_dim = 128,         # stage 2 (same as above)\n",
        "#     s2_patch_size = 2,\n",
        "#     s2_local_patch_size = 7,\n",
        "#     s2_global_k = 7,\n",
        "#     s2_depth = 1,\n",
        "#     s3_emb_dim = 256,         # stage 3 (same as above)\n",
        "#     s3_patch_size = 2,\n",
        "#     s3_local_patch_size = 7,\n",
        "#     s3_global_k = 7,\n",
        "#     s3_depth = 5,\n",
        "#     s4_emb_dim = 512,         # stage 4 (same as above)\n",
        "#     s4_patch_size = 2,\n",
        "#     s4_local_patch_size = 7,\n",
        "#     s4_global_k = 7,\n",
        "#     s4_depth = 4,\n",
        "#     peg_kernel_size = 3,      # positional encoding generator kernel size\n",
        "#     dropout = 0.              # dropout\n",
        "# )\n",
        "\n",
        "# img = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# pred = model(img) # (1, 1000)\n",
        "# print('preds',preds.shape)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2WS8pF9hgCY"
      },
      "source": [
        "# import torch\n",
        "# from vit_pytorch.regionvit import RegionViT\n",
        "\n",
        "# model = RegionViT(\n",
        "#     dim = (64, 128, 256, 512),      # tuple of size 4, indicating dimension at each stage\n",
        "#     depth = (2, 2, 8, 2),           # depth of the region to local transformer at each stage\n",
        "#     window_size = 7,                # window size, which should be either 7 or 14\n",
        "#     num_classes = 2000,             # number of output lcasses\n",
        "#     tokenize_local_3_conv = False,  # whether to use a 3 layer convolution to encode the local tokens from the image. the paper uses this for the smaller models, but uses only 1 conv (set to False) for the larger models\n",
        "#     use_peg = False,                # whether to use positional generating module. they used this for object detection for a boost in performance\n",
        "# )\n",
        "\n",
        "# img = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# pred = model(img) # (1, 1000)\n",
        "# print('preds',preds.shape)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZkPQQG0hrFE",
        "outputId": "e35472f1-7aee-432d-8ef1-7fec30251677"
      },
      "source": [
        "import torch\n",
        "from vit_pytorch.nest import NesT\n",
        "\n",
        "nest = NesT(\n",
        "    image_size = 32,\n",
        "    patch_size = 8,\n",
        "    dim = 96,\n",
        "    heads = 9,\n",
        "    num_hierarchies = 3,        # number of hierarchies\n",
        "    block_repeats = (8, 4, 1),  # the number of transformer blocks at each heirarchy, starting from the bottom\n",
        "    num_classes = 2000\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 32, 32)\n",
        "\n",
        "pred = nest(img) # (1, 1000)\n",
        "print('preds',preds.shape)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preds torch.Size([1, 2000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXCq4TD7hxTH"
      },
      "source": [
        "# import torch\n",
        "# from vit_pytorch import ViT\n",
        "# from vit_pytorch.mpp import MPP\n",
        "\n",
        "# model = ViT(\n",
        "#     image_size=32,\n",
        "#     patch_size=8,\n",
        "#     num_classes=1000,\n",
        "#     dim=1024,\n",
        "#     depth=8,\n",
        "#     heads=16,\n",
        "#     mlp_dim=2048,\n",
        "#     dropout=0.1,\n",
        "#     emb_dropout=0.1\n",
        "# )\n",
        "\n",
        "# mpp_trainer = MPP(\n",
        "#     transformer=model,\n",
        "#     patch_size=8,\n",
        "#     dim=1024,\n",
        "#     mask_prob=0.15,          # probability of using token in masked prediction task\n",
        "#     random_patch_prob=0.30,  # probability of randomly replacing a token being used for mpp\n",
        "#     replace_prob=0.50,       # probability of replacing a token being used for mpp with the mask token\n",
        "# )\n",
        "\n",
        "# opt = torch.optim.Adam(mpp_trainer.parameters(), lr=3e-4)\n",
        "\n",
        "# def sample_unlabelled_images():\n",
        "#     return torch.FloatTensor(20, 3, 32, 32).uniform_(0., 1.)\n",
        "\n",
        "# for _ in range(1):\n",
        "#     images = sample_unlabelled_images()\n",
        "#     loss = mpp_trainer(images)\n",
        "#     opt.zero_grad()\n",
        "#     loss.backward()\n",
        "#     opt.step()\n",
        "\n",
        "# # save your improved network\n",
        "# torch.save(model.state_dict(), './pretrained-net.pt')\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOjT65Wth8wu"
      },
      "source": [
        "# import torch\n",
        "# from vit_pytorch import ViT, Dino\n",
        "\n",
        "# model = ViT(\n",
        "#     image_size = 256,\n",
        "#     patch_size = 32,\n",
        "#     num_classes = 1000,\n",
        "#     dim = 1024,\n",
        "#     depth = 6,\n",
        "#     heads = 8,\n",
        "#     mlp_dim = 2048\n",
        "# )\n",
        "\n",
        "# learner = Dino(\n",
        "#     model,\n",
        "#     image_size = 256,\n",
        "#     hidden_layer = 'to_latent',        # hidden layer name or index, from which to extract the embedding\n",
        "#     projection_hidden_size = 256,      # projector network hidden dimension\n",
        "#     projection_layers = 4,             # number of layers in projection network\n",
        "#     num_classes_K = 65336,             # output logits dimensions (referenced as K in paper)\n",
        "#     student_temp = 0.9,                # student temperature\n",
        "#     teacher_temp = 0.04,               # teacher temperature, needs to be annealed from 0.04 to 0.07 over 30 epochs\n",
        "#     local_upper_crop_scale = 0.4,      # upper bound for local crop - 0.4 was recommended in the paper \n",
        "#     global_lower_crop_scale = 0.5,     # lower bound for global crop - 0.5 was recommended in the paper\n",
        "#     moving_average_decay = 0.9,        # moving average of encoder - paper showed anywhere from 0.9 to 0.999 was ok\n",
        "#     center_moving_average_decay = 0.9, # moving average of teacher centers - paper showed anywhere from 0.9 to 0.999 was ok\n",
        "# )\n",
        "\n",
        "# opt = torch.optim.Adam(learner.parameters(), lr = 3e-4)\n",
        "\n",
        "# def sample_unlabelled_images():\n",
        "#     return torch.randn(20, 3, 256, 256)\n",
        "\n",
        "# for _ in range(1):\n",
        "#     images = sample_unlabelled_images()\n",
        "#     loss = learner(images)\n",
        "#     opt.zero_grad()\n",
        "#     loss.backward()\n",
        "#     opt.step()\n",
        "#     learner.update_moving_average() # update moving average of teacher encoder and teacher centers\n",
        "\n",
        "# # save your improved network\n",
        "# torch.save(model.state_dict(), './pretrained-net.pt')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJW_xfOTikQM",
        "outputId": "78b97938-0d60-43e6-d03a-4c726c519b24"
      },
      "source": [
        "import torch\n",
        "from vit_pytorch.vit import ViT\n",
        "\n",
        "v = ViT(\n",
        "    image_size = 32,\n",
        "    patch_size = 8,\n",
        "    num_classes = 2000,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")\n",
        "\n",
        "# import Recorder and wrap the ViT\n",
        "\n",
        "from vit_pytorch.recorder import Recorder\n",
        "v = Recorder(v)\n",
        "\n",
        "# forward pass now returns predictions and the attention maps\n",
        "\n",
        "img = torch.randn(1, 3, 32, 32)\n",
        "preds, attns = v(img)\n",
        "\n",
        "print('preds',preds.shape)\n",
        "\n",
        "# there is one extra patch due to the CLS token\n",
        "\n",
        "print('attns',attns.shape) # (1, 6, 16, 65, 65) - (batch x layers x heads x patch x patch)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preds torch.Size([1, 2000])\n",
            "attns torch.Size([1, 6, 16, 17, 17])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avMjZ7Giiso5"
      },
      "source": [
        "####research\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKvjuonmizzR",
        "outputId": "884b4794-9500-4971-f8ee-b0732bee9e87"
      },
      "source": [
        "pip install nystrom-attention\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nystrom-attention in /usr/local/lib/python3.7/dist-packages (0.0.11)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.7/dist-packages (from nystrom-attention) (0.3.2)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from nystrom-attention) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->nystrom-attention) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgR0WIH8i9_l",
        "outputId": "35c5844c-5b34-45ed-f730-1b6968633fb8"
      },
      "source": [
        "import torch\n",
        "from vit_pytorch.efficient import ViT\n",
        "from nystrom_attention import Nystromformer\n",
        "\n",
        "efficient_transformer = Nystromformer(\n",
        "    dim = 512,\n",
        "    depth = 12,\n",
        "    heads = 16,\n",
        "    num_landmarks = 256\n",
        ")\n",
        "\n",
        "v = ViT(\n",
        "    dim = 512,\n",
        "    image_size = 32,\n",
        "    patch_size = 8,\n",
        "    num_classes = 2000,\n",
        "    transformer = efficient_transformer\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 32, 32) # your high resolution picture\n",
        "preds = v(img) # (1, 1000)\n",
        "print('preds',preds.shape)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preds torch.Size([1, 2000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvV3L1mPjDZM",
        "outputId": "8c551d4c-e193-4c12-f274-4f61dd64c2be"
      },
      "source": [
        "!pip install x-transformers"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: x-transformers in /usr/local/lib/python3.7/dist-packages (0.20.5)\n",
            "Requirement already satisfied: entmax in /usr/local/lib/python3.7/dist-packages (from x-transformers) (1.0)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.7/dist-packages (from x-transformers) (0.3.2)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from x-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->x-transformers) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcaxbiYTjS12",
        "outputId": "44f82901-67a2-4808-de50-a00e8f001934"
      },
      "source": [
        "import torch\n",
        "from vit_pytorch.efficient import ViT\n",
        "from x_transformers import Encoder\n",
        "\n",
        "v = ViT(\n",
        "    dim = 512,\n",
        "    image_size = 32,\n",
        "    patch_size = 8,\n",
        "    num_classes = 2000,\n",
        "    transformer = Encoder(\n",
        "        dim = 512,                  # set to be the same as the wrapper\n",
        "        depth = 12,\n",
        "        heads = 16,\n",
        "        ff_glu = True,              # ex. feed forward GLU variant https://arxiv.org/abs/2002.05202\n",
        "        residual_attn = True        # ex. residual attention https://arxiv.org/abs/2012.11747\n",
        "    )\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 32, 32)\n",
        "preds = v(img) # (1, 1000)\n",
        "print('preds',preds.shape)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preds torch.Size([1, 2000])\n"
          ]
        }
      ]
    }
  ]
}